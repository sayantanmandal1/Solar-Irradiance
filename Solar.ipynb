{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNNtxCCfwUsri35TnVN97U9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sayantanmandal1/Solar-Irradiance/blob/main/Solar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hogY2F6h-l7j",
        "outputId": "0bd36abc-a8e2-4848-ce33-e38a3291ae20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-09 16:17:05--  https://raw.githubusercontent.com/sayantanmandal1/Solar-Irradiance/refs/heads/main/interpolated_dataset_1000.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14954 (15K) [text/plain]\n",
            "Saving to: ‘interpolated_dataset_1000.csv’\n",
            "\n",
            "interpolated_datase 100%[===================>]  14.60K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2024-11-09 16:17:05 (9.34 MB/s) - ‘interpolated_dataset_1000.csv’ saved [14954/14954]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "!wget https://raw.githubusercontent.com/sayantanmandal1/Solar-Irradiance/refs/heads/main/interpolated_dataset_1000.csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('interpolated_dataset_1000.csv')\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "O2IeN7Ld--KY",
        "outputId": "855db82f-9147-4425-85cc-c678ab4d7290"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           X      Y\n",
              "0     49.100  2.090\n",
              "1     49.515  2.088\n",
              "2     49.930  2.085\n",
              "3     50.346  2.083\n",
              "4     50.761  2.080\n",
              "..       ...    ...\n",
              "995  257.119  2.035\n",
              "996  255.989  2.034\n",
              "997  254.859  2.032\n",
              "998  253.730  2.031\n",
              "999  252.600  2.030\n",
              "\n",
              "[1000 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8b65da05-2c34-4726-8656-e964bca4fc34\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>X</th>\n",
              "      <th>Y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>49.100</td>\n",
              "      <td>2.090</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>49.515</td>\n",
              "      <td>2.088</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>49.930</td>\n",
              "      <td>2.085</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>50.346</td>\n",
              "      <td>2.083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>50.761</td>\n",
              "      <td>2.080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>257.119</td>\n",
              "      <td>2.035</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>255.989</td>\n",
              "      <td>2.034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>254.859</td>\n",
              "      <td>2.032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>253.730</td>\n",
              "      <td>2.031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>252.600</td>\n",
              "      <td>2.030</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8b65da05-2c34-4726-8656-e964bca4fc34')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8b65da05-2c34-4726-8656-e964bca4fc34 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8b65da05-2c34-4726-8656-e964bca4fc34');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b51ae300-a058-4f7d-bfd1-8acecb765a1d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b51ae300-a058-4f7d-bfd1-8acecb765a1d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b51ae300-a058-4f7d-bfd1-8acecb765a1d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_c37d6cb4-1b17-477f-af34-0cf4be00b37a\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_c37d6cb4-1b17-477f-af34-0cf4be00b37a button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 1000,\n  \"fields\": [\n    {\n      \"column\": \"X\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 246.67712262291838,\n        \"min\": 49.1,\n        \"max\": 777.297,\n        \"num_unique_values\": 998,\n        \"samples\": [\n          464.546,\n          114.981,\n          765.114\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Y\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.10544286651119694,\n        \"min\": 1.85,\n        \"max\": 2.411,\n        \"num_unique_values\": 355,\n        \"samples\": [\n          2.035,\n          2.41,\n          2.038\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "DMg8Epk__KhB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=pd.read_csv('interpolated_dataset_1000.csv')"
      ],
      "metadata": {
        "id": "UveCBYFT_cJW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = data[['X']].values\n",
        "Y = data['Y'].values\n",
        "\n",
        "# Use Y as the new input and X as the new output\n",
        "scaler_Y = MinMaxScaler()\n",
        "scaler_X = MinMaxScaler()\n",
        "Y_scaled = scaler_Y.fit_transform(Y.reshape(-1, 1))\n",
        "X_scaled = scaler_X.fit_transform(X)\n",
        "\n",
        "# Build and compile the model\n",
        "model = Sequential([\n",
        "    Dense(64, input_dim=1, activation='relu'),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='linear')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "model.fit(Y_scaled, X_scaled, epochs=200, verbose=0)\n",
        "\n",
        "# Predict and evaluate\n",
        "X_pred_scaled = model.predict(Y_scaled)\n",
        "X_pred = scaler_X.inverse_transform(X_pred_scaled)\n",
        "rmse = np.sqrt(mean_squared_error(X, X_pred))\n",
        "\n",
        "print(\"Predicted X values:\", X_pred[:5])\n",
        "print(\"RMSE:\", rmse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3_Ju51t_frl",
        "outputId": "e55a661c-d371-43b2-a117-13ff303ae71f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "Predicted X values: [[279.92957]\n",
            " [281.19827]\n",
            " [283.1014 ]\n",
            " [284.3739 ]\n",
            " [286.2962 ]]\n",
            "RMSE: 207.66003339525884\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "X = data[['X']].values\n",
        "Y = data['Y'].values\n",
        "\n",
        "# Swap input and output (Y -> X)\n",
        "scaler_Y = MinMaxScaler()\n",
        "scaler_X = MinMaxScaler()\n",
        "Y_scaled = scaler_Y.fit_transform(Y.reshape(-1, 1))\n",
        "X_scaled = scaler_X.fit_transform(X)\n",
        "\n",
        "# Build a more complex neural network model\n",
        "model = Sequential([\n",
        "    Dense(128, input_dim=1, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1, activation='linear')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "model.fit(Y_scaled, X_scaled, epochs=1000, batch_size=32, verbose=0)\n",
        "\n",
        "# Predict and evaluate\n",
        "X_pred_scaled = model.predict(Y_scaled)\n",
        "X_pred = scaler_X.inverse_transform(X_pred_scaled)\n",
        "rmse = np.sqrt(mean_squared_error(X, X_pred))\n",
        "\n",
        "print(\"Predicted X values:\", X_pred[:5])\n",
        "print(\"RMSE:\", rmse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVYGo1uLAY9S",
        "outputId": "10eb5056-3502-4229-c377-3a568dde7a1e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
            "Predicted X values: [[271.78662]\n",
            " [271.5971 ]\n",
            " [271.32074]\n",
            " [271.1579 ]\n",
            " [270.9136 ]]\n",
            "RMSE: 204.337076234774\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Load your dataset\n",
        "X = data[['X']].values  # Reshape X for sklearn's requirement\n",
        "Y = data['Y'].values\n",
        "\n",
        "# Define and fit the polynomial regression model\n",
        "degree = 8\n",
        "poly = PolynomialFeatures(degree)\n",
        "X_poly = poly.fit_transform(X)\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, Y)\n",
        "\n",
        "# Construct the polynomial equation\n",
        "coeffs = model.coef_\n",
        "intercept = model.intercept_\n",
        "equation = \"Y = \" + \" + \".join([f\"{coeffs[i]:.8f} * X^{degree-i}\" for i in range(1, degree+1)]) + f\" + {intercept:.8f}\"\n",
        "print(\"Regression Equation:\", equation)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Oey5KJktkPp",
        "outputId": "988f71e0-ae8d-49a0-f94b-f66c13d92687"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Regression Equation: Y = -0.00000000 * X^7 + -0.00000000 * X^6 + -0.00000000 * X^5 + -0.00000000 * X^4 + 0.00000000 * X^3 + -0.00000000 * X^2 + 0.00000000 * X^1 + -0.00000000 * X^0 + 2.13185224\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Load your dataset\n",
        "Y = data[['Y']].values  # Now treating Y as input\n",
        "X = data['X'].values    # Treating X as the target\n",
        "\n",
        "# Fit a polynomial regression model to predict X from Y\n",
        "degree = 8\n",
        "poly = PolynomialFeatures(degree)\n",
        "Y_poly = poly.fit_transform(Y)\n",
        "model = LinearRegression()\n",
        "model.fit(Y_poly, X)\n",
        "\n",
        "# Construct the regression equation for predicting X from Y\n",
        "coeffs = model.coef_\n",
        "intercept = model.intercept_\n",
        "equation = \"X = \" + \" + \".join([f\"{coeffs[i]:.8f} * Y^{degree-i}\" for i in range(1, degree+1)]) + f\" + {intercept:.8f}\"\n",
        "print(\"Regression Equation for X as a function of Y:\", equation)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohCSyjQbuuoC",
        "outputId": "f1db90dd-3357-4bc9-a5fa-2e688d5b4e9a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Regression Equation for X as a function of Y: X = 17957429086.45363998 * Y^7 + -32045962948.21349716 * Y^6 + 32459376832.61334229 * Y^5 + -20423868093.28002548 * Y^4 + 8178886315.74519730 * Y^3 + -2036570776.77263260 * Y^2 + 288401916.37435722 * Y^1 + -17788766.18932724 * Y^0 + -4369484768.95116234\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv('interpolated_dataset_1000.csv')\n",
        "\n",
        "# Separate features (Y) and target (X)\n",
        "X = data[['Y']].values  # LDR values as features (input)\n",
        "y = data['X'].values    # Solar irradiance as target (output)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalize data for neural network\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "X_train_scaled = scaler_X.fit_transform(X_train)\n",
        "X_test_scaled = scaler_X.transform(X_test)\n",
        "y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1))\n",
        "y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1))\n",
        "\n",
        "# Define neural network model\n",
        "model = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1)  # Single output for regression\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_scaled, y_train_scaled, epochs=100, batch_size=16, validation_split=0.2, verbose=0)\n",
        "\n",
        "# Predict on test data and inverse transform to get predictions in original scale\n",
        "y_pred_scaled = model.predict(X_test_scaled)\n",
        "y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
        "\n",
        "# Calculating Mean Absolute Error to evaluate the model's performance\n",
        "mae = np.mean(np.abs(y_test - y_pred.flatten()))\n",
        "\n",
        "mae\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrBUZfDBwHT8",
        "outputId": "7f1b781b-d2aa-4be8-f480-3c8bdd6cac40"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "170.02779454833984"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "\n",
        "\n",
        "# Separate features (Y) and target (X)\n",
        "X = data[['Y']].values  # LDR values as features (input)\n",
        "y = data['X'].values    # Solar irradiance as target (output)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalize data for neural network\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "X_train_scaled = scaler_X.fit_transform(X_train)\n",
        "X_test_scaled = scaler_X.transform(X_test)\n",
        "y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1))\n",
        "y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1))\n",
        "\n",
        "# Define neural network model\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),  # Increased neurons\n",
        "    Dropout(0.3),  # Dropout layer to prevent overfitting\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.3),  # Dropout layer\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1)  # Single output for regression\n",
        "])\n",
        "\n",
        "# Compile model with RMSprop optimizer\n",
        "model.compile(optimizer=RMSprop(learning_rate=0.0005), loss='mse')\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_scaled, y_train_scaled, epochs=200, batch_size=32, validation_split=0.2, verbose=0)\n",
        "\n",
        "# Predict on test data and inverse transform to get predictions in original scale\n",
        "y_pred_scaled = model.predict(X_test_scaled)\n",
        "y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
        "\n",
        "# Calculating Mean Absolute Error to evaluate the model's performance\n",
        "mae = np.mean(np.abs(y_test - y_pred.flatten()))\n",
        "\n",
        "mae\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WOPFVlvwmKm",
        "outputId": "69a03b8e-7c98-48d4-800a-1e60b529b3ff"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "WARNING:tensorflow:5 out of the last 40 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7e6d34664700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "176.9947965661621"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Train-test split (same as before)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Random Forest model\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Calculate Mean Absolute Error\n",
        "mae_rf = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "mae_rf\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ahu_r6Ifw2Ql",
        "outputId": "e03593e6-75da-452e-c2ff-4bd8caadce89"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "175.8784889719727"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Train-test split (same as before)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize XGBoost model\n",
        "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, learning_rate=0.01, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = xgb_model.predict(X_test)\n",
        "\n",
        "# Calculate Mean Absolute Error\n",
        "mae_xgb = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "mae_xgb\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcxlB047w8eK",
        "outputId": "2cacf334-7e1d-48d0-8d2c-f52e9c304a4d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "181.75612549865724"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Initialize Random Forest model\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# Apply K-Fold Cross-validation\n",
        "cv_scores = cross_val_score(rf_model, X, y, cv=5, scoring='neg_mean_absolute_error')\n",
        "\n",
        "# Calculate the average MAE\n",
        "average_cv_score = -cv_scores.mean()  # Negative because scores are returned as negative\n",
        "average_cv_score\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0q_i9gBxITc",
        "outputId": "baf5b5ac-be7f-47c6-996d-164f437c8dcc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "275.90749375196253"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Initialize Linear Regression model\n",
        "lr_model = LinearRegression()\n",
        "\n",
        "# Train the model\n",
        "lr_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_lr = lr_model.predict(X_test)\n",
        "\n",
        "# Calculate Mean Absolute Error\n",
        "mae_lr = mean_absolute_error(y_test, y_pred_lr)\n",
        "\n",
        "mae_lr\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRl6npu0xJ7M",
        "outputId": "fc607eb8-2d95-4caf-ce89-c1dfa1ea584e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "203.7190923314218"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Initialize Linear Regression model\n",
        "lr_model = LinearRegression()\n",
        "\n",
        "# Train the model\n",
        "lr_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_lr = lr_model.predict(X_test)\n",
        "\n",
        "# Calculate Mean Absolute Error\n",
        "mae_lr = mean_absolute_error(y_test, y_pred_lr)\n",
        "\n",
        "mae_lr\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gw68iekWxQ-a",
        "outputId": "aa2ad192-6293-460b-bcc3-92a1860c766d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "203.7190923314218"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "# Define a model with Dropout\n",
        "model = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
        "    Dropout(0.2),  # Dropout layer added\n",
        "    Dense(32, activation='relu'),\n",
        "    Dropout(0.2),  # Dropout layer added\n",
        "    Dense(1)  # Single output for regression\n",
        "])\n",
        "\n",
        "# Compile and train the model as before\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
        "history = model.fit(X_train_scaled, y_train_scaled, epochs=100, batch_size=16, validation_split=0.2, verbose=0)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_scaled = model.predict(X_test_scaled)\n",
        "y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
        "mae_nn = np.mean(np.abs(y_test - y_pred.flatten()))\n",
        "\n",
        "mae_nn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyvaVdmSxV0r",
        "outputId": "4f87c532-2a39-4952-f6df-4b6212a4ed5a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "WARNING:tensorflow:5 out of the last 15 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7e6d37ac88b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "171.57408023193358"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "\n",
        "# Calculate Z-scores\n",
        "z_scores = stats.zscore(data[['Y', 'X']])\n",
        "\n",
        "# Remove rows where Z-score > 3 or < -3 (outliers)\n",
        "data_no_outliers = data[(z_scores < 3).all(axis=1)]\n",
        "\n",
        "# Re-run the model after cleaning the data\n"
      ],
      "metadata": {
        "id": "zkDqQOdexbex"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Initialize models\n",
        "models = {\n",
        "    'Linear Regression': LinearRegression(),\n",
        "    'Random Forest': RandomForestRegressor(),\n",
        "    'SVR': SVR()\n",
        "}\n",
        "\n",
        "# Evaluate each model using cross-validation\n",
        "for name, model in models.items():\n",
        "    scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_absolute_error')\n",
        "    print(f\"{name} MAE: {-scores.mean()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAV4P1Qdxj8y",
        "outputId": "506b76ca-2cff-4010-84ce-69e6cd29aac5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Regression MAE: 285.1805649089878\n",
            "Random Forest MAE: 276.4156213683883\n",
            "SVR MAE: 307.0112848214276\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "# Redefine model with Dropout layers and smaller learning rate\n",
        "model = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
        "    Dropout(0.2),  # Add Dropout\n",
        "    Dense(32, activation='relu'),\n",
        "    Dropout(0.2),  # Add Dropout\n",
        "    Dense(1)  # Single output for regression\n",
        "])\n",
        "\n",
        "# Compile and train the model with a smaller learning rate\n",
        "model.compile(optimizer=Adam(learning_rate=0.0005), loss='mse')\n",
        "history = model.fit(X_train_scaled, y_train_scaled, epochs=100, batch_size=16, validation_split=0.2, verbose=0)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_scaled = model.predict(X_test_scaled)\n",
        "y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
        "mae_nn = np.mean(np.abs(y_test - y_pred.flatten()))\n",
        "\n",
        "mae_nn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsSGqTUQxrMd",
        "outputId": "e6272c7b-fd25-48c3-f9c1-657be3429785"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "175.04148985595705"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define parameter grid for Random Forest\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [10, 20, None],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2]\n",
        "}\n",
        "\n",
        "# Perform Grid Search\n",
        "grid_search = GridSearchCV(estimator=RandomForestRegressor(), param_grid=param_grid, cv=5, scoring='neg_mean_absolute_error')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters and best MAE\n",
        "print(\"Best Parameters: \", grid_search.best_params_)\n",
        "print(\"Best MAE: \", -grid_search.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5dChrPIx6mH",
        "outputId": "5c3c5ee5-2e8b-40da-b3ec-86402fe24928"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters:  {'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 300}\n",
            "Best MAE:  172.132493935024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Define early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "# Train with early stopping\n",
        "history = model.fit(X_train_scaled, y_train_scaled, epochs=100, batch_size=16, validation_split=0.2, verbose=0, callbacks=[early_stopping])\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_scaled = model.predict(X_test_scaled)\n",
        "y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
        "mae_nn = np.mean(np.abs(y_test - y_pred.flatten()))\n",
        "mae_nn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znPJmpRByAmF",
        "outputId": "7b2d4eae-ccd4-4ad0-a3a9-6dbf71c4e1eb"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "173.66776045532228"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Best Random Forest model with tuned parameters\n",
        "rf_best_model = RandomForestRegressor(\n",
        "    n_estimators=300,\n",
        "    max_depth=10,\n",
        "    min_samples_split=2,\n",
        "    min_samples_leaf=2\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "rf_best_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred_rf_best = rf_best_model.predict(X_test)\n",
        "\n",
        "# Calculate MAE\n",
        "mae_rf_best = mean_absolute_error(y_test, y_pred_rf_best)\n",
        "print(\"Final MAE with Best Parameters: \", mae_rf_best)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkK6iOUzyxFf",
        "outputId": "68aa5979-03b6-466c-b219-967357111cb6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final MAE with Best Parameters:  169.45304376039837\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rf_best_model.fit(X_train, y_train)\n",
        "y_pred_rf_best = rf_best_model.predict(X_test)\n",
        "mae_rf_best = mean_absolute_error(y_test, y_pred_rf_best)\n",
        "print(\"Final MAE with Best Parameters (Random Forest):\", mae_rf_best)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AsNhED1Syypk",
        "outputId": "d24e99a1-08b4-4395-e852-8bad0673ebba"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final MAE with Best Parameters (Random Forest): 169.40197612238967\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "param_dist = {\n",
        "    'n_estimators': [100, 200, 300, 500],\n",
        "    'max_depth': [5, 10, 15, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "rf_model = RandomForestRegressor()\n",
        "\n",
        "# RandomizedSearchCV to explore hyperparameters\n",
        "random_search = RandomizedSearchCV(rf_model, param_distributions=param_dist, n_iter=100, cv=5, verbose=2, random_state=42, n_jobs=-1)\n",
        "\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "best_rf_model = random_search.best_estimator_\n",
        "y_pred_rf = best_rf_model.predict(X_test)\n",
        "\n",
        "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
        "print(\"Best Random Forest MAE after tuning:\", mae_rf)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-QWgcMLzFwY",
        "outputId": "a688f798-c52b-4d16-ff89-cb60e4a724e0"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/ma/core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
            "  _data = np.array(data, dtype=dtype, copy=copy,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Random Forest MAE after tuning: 158.70912342202283\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Learning Rate Scheduler in the neural network\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "\n",
        "def scheduler(epoch, lr):\n",
        "    if epoch < 10:\n",
        "        return lr\n",
        "    else:\n",
        "        return lr * tf.math.exp(-0.1)\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(scheduler)\n",
        "\n",
        "history = model.fit(X_train_scaled, y_train_scaled, epochs=100, batch_size=16, validation_split=0.2, callbacks=[lr_scheduler], verbose=0)\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "rf_model = RandomForestRegressor(n_estimators=300, max_depth=10, min_samples_leaf=2)\n",
        "\n",
        "# Use cross-validation to get more reliable results\n",
        "cv_scores = cross_val_score(rf_model, X_train, y_train, cv=5, scoring='neg_mean_absolute_error')\n",
        "print(\"Cross-validated MAE: \", -cv_scores.mean())\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X_train)\n",
        "from tensorflow.keras.layers import Dropout, BatchNormalization\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.2),\n",
        "    Dense(32, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.2),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "gb_model = GradientBoostingRegressor(n_estimators=100, max_depth=10)\n",
        "gb_model.fit(X_train, y_train)\n",
        "y_pred_gb = gb_model.predict(X_test)\n",
        "\n",
        "mae_gb = mean_absolute_error(y_test, y_pred_gb)\n",
        "print(\"Gradient Boosting MAE: \", mae_gb)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "zrgNEuuazL9h",
        "outputId": "856b0820-6146-4baf-8a52-555410fb5df7"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "The output of the `schedule` function should be a float. Got: 5.583294893085622e-08",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-f552288b093e>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mlr_scheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLearningRateScheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/callbacks/learning_rate_scheduler.py\u001b[0m in \u001b[0;36mon_epoch_begin\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m     66\u001b[0m                 \u001b[0;34m\"The output of the `schedule` function should be a float. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0;34mf\"Got: {learning_rate}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The output of the `schedule` function should be a float. Got: 5.583294893085622e-08"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "\n",
        "def scheduler(epoch, lr):\n",
        "    if epoch < 10:\n",
        "        return float(lr)  # Ensure the learning rate is returned as a float\n",
        "    else:\n",
        "        return float(lr * tf.math.exp(-0.1))  # Ensure the result is a float\n",
        "\n",
        "# Learning rate scheduler\n",
        "lr_scheduler = LearningRateScheduler(scheduler)\n",
        "\n",
        "# Fit the model with the scheduler\n",
        "history = model.fit(X_train_scaled, y_train_scaled, epochs=100, batch_size=16, validation_split=0.2, callbacks=[lr_scheduler], verbose=0)\n"
      ],
      "metadata": {
        "id": "uDDW1eeW0eKE"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def scheduler(epoch, lr):\n",
        "    if epoch < 10:\n",
        "        return float(lr)  # Ensure the learning rate is returned as a float\n",
        "    else:\n",
        "        return float(lr * tf.math.exp(-0.1))  # Ensure the result is a float\n",
        "\n",
        "# Create the learning rate scheduler\n",
        "lr_scheduler = LearningRateScheduler(scheduler)\n",
        "\n",
        "# Fit the neural network with learning rate scheduling\n",
        "history = model.fit(X_train_scaled, y_train_scaled, epochs=100, batch_size=16, validation_split=0.2, callbacks=[lr_scheduler], verbose=0)\n"
      ],
      "metadata": {
        "id": "Nbh7uKI21HE3"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "rf_model = RandomForestRegressor(n_estimators=300, max_depth=10, min_samples_leaf=2)\n",
        "\n",
        "# Use cross-validation to get more reliable results\n",
        "cv_scores = cross_val_score(rf_model, X_train, y_train, cv=5, scoring='neg_mean_absolute_error')\n",
        "print(\"Cross-validated MAE: \", -cv_scores.mean())\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X_train)\n",
        "from tensorflow.keras.layers import Dropout, BatchNormalization\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.2),\n",
        "    Dense(32, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.2),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "gb_model = GradientBoostingRegressor(n_estimators=100, max_depth=10)\n",
        "gb_model.fit(X_train, y_train)\n",
        "y_pred_gb = gb_model.predict(X_test)\n",
        "\n",
        "mae_gb = mean_absolute_error(y_test, y_pred_gb)\n",
        "print(\"Gradient Boosting MAE: \", mae_gb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CypmTNH1OeD",
        "outputId": "4bd09ae2-a50a-493b-a107-a372630fa743"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validated MAE:  172.00466890907632\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Boosting MAE:  181.86792362603344\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
        "\n",
        "# Define the model with Input layer\n",
        "model = Sequential([\n",
        "    Input(shape=(X_train_scaled.shape[1],)),  # Input layer\n",
        "    Dense(64, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.2),\n",
        "    Dense(32, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.2),\n",
        "    Dense(1)  # Output layer\n",
        "])\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n"
      ],
      "metadata": {
        "id": "HXS5e0tv1Y-y"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "gb_model = GradientBoostingRegressor(n_estimators=100, max_depth=10)\n",
        "gb_model.fit(X_train, y_train)\n",
        "y_pred_gb = gb_model.predict(X_test)\n",
        "\n",
        "mae_gb = mean_absolute_error(y_test, y_pred_gb)\n",
        "print(\"Gradient Boosting MAE: \", mae_gb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YJnFUOB1wDX",
        "outputId": "c1cfc5ad-36ea-430f-cddd-704eeff15457"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Boosting MAE:  181.86792362603344\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "# Define parameter grid for GradientBoostingRegressor\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [3, 5, 10],\n",
        "    'learning_rate': [0.01, 0.05, 0.1]\n",
        "}\n",
        "\n",
        "# Initialize the model\n",
        "gb_model = GradientBoostingRegressor()\n",
        "\n",
        "# Use GridSearchCV to find the best parameters\n",
        "grid_search = GridSearchCV(gb_model, param_grid, cv=5, scoring='neg_mean_absolute_error')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and score\n",
        "best_params = grid_search.best_params_\n",
        "best_mae = -grid_search.best_score_\n",
        "\n",
        "print(\"Best Gradient Boosting Parameters: \", best_params)\n",
        "print(\"Best MAE: \", best_mae)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FE_Fp83d12J4",
        "outputId": "edbe5c6a-8d7d-4c63-8d87-b030fdcb174d"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Gradient Boosting Parameters:  {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 100}\n",
            "Best MAE:  168.45505766329464\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_train_poly = poly.fit_transform(X_train)\n",
        "\n",
        "# Train a Gradient Boosting model on the new features\n",
        "gb_model.fit(X_train_poly, y_train)\n",
        "y_pred_gb = gb_model.predict(X_test)\n",
        "mae_gb_poly = mean_absolute_error(y_test, y_pred_gb)\n",
        "print(\"Gradient Boosting MAE with Polynomial Features: \", mae_gb_poly)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "jk2U5UPq2aXx",
        "outputId": "0577c464-b66f-4eb9-8aa2-01ea9ef9e1fa"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "X has 1 features, but GradientBoostingRegressor is expecting 3 features as input.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-986dcb0821ae>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Train a Gradient Boosting model on the new features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mgb_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_poly\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0my_pred_gb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgb_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mmae_gb_poly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_gb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Gradient Boosting MAE with Polynomial Features: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmae_gb_poly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   2125\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2126\u001b[0m         \"\"\"\n\u001b[0;32m-> 2127\u001b[0;31m         X = self._validate_data(\n\u001b[0m\u001b[1;32m   2128\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2129\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcheck_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ensure_2d\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 654\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_n_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_in_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    444\u001b[0m                 \u001b[0;34mf\"X has {n_features} features, but {self.__class__.__name__} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m                 \u001b[0;34mf\"is expecting {self.n_features_in_} features as input.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: X has 1 features, but GradientBoostingRegressor is expecting 3 features as input."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Apply PolynomialFeatures transformation to both train and test data\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_train_poly = poly.fit_transform(X_train)\n",
        "X_test_poly = poly.transform(X_test)\n",
        "\n",
        "# Train a Gradient Boosting model on the transformed training features\n",
        "gb_model.fit(X_train_poly, y_train)\n",
        "\n",
        "# Make predictions on the transformed test data\n",
        "y_pred_gb = gb_model.predict(X_test_poly)\n",
        "\n",
        "# Calculate the MAE for Gradient Boosting on the transformed features\n",
        "mae_gb_poly = mean_absolute_error(y_test, y_pred_gb)\n",
        "print(\"Gradient Boosting MAE with Polynomial Features: \", mae_gb_poly)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JI99lS5v2lMS",
        "outputId": "00fa3a66-b985-4533-bbcc-285d89241644"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Boosting MAE with Polynomial Features:  162.5958560208977\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Polynomial feature transformation\n",
        "poly = PolynomialFeatures(degree=3)\n",
        "X_poly = poly.fit_transform(X_train)\n",
        "\n",
        "# Gradient Boosting model with Grid Search for tuning\n",
        "params = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [3, 5, 10],\n",
        "    'learning_rate': [0.01, 0.05, 0.1]\n",
        "}\n",
        "\n",
        "gb_model = GradientBoostingRegressor()\n",
        "grid_search = GridSearchCV(gb_model, param_grid=params, cv=5, scoring='neg_mean_absolute_error', verbose=2)\n",
        "grid_search.fit(X_poly, y_train)\n",
        "\n",
        "# Best parameters from grid search\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Predict and evaluate MAE\n",
        "y_pred = best_model.predict(X_test)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(\"Final MAE after Grid Search: \", mae)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "t81Sw3a32mIO",
        "outputId": "3849a3a0-8880-4f94-f1ff-d730245c88d5"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
            "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=100; total time=   0.4s\n",
            "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=100; total time=   0.4s\n",
            "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=100; total time=   0.3s\n",
            "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=100; total time=   0.2s\n",
            "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=100; total time=   0.2s\n",
            "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=200; total time=   0.4s\n",
            "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=200; total time=   0.3s\n",
            "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=200; total time=   0.4s\n",
            "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=200; total time=   0.3s\n",
            "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=200; total time=   0.3s\n",
            "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=300; total time=   0.6s\n",
            "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=300; total time=   0.6s\n",
            "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=300; total time=   0.7s\n",
            "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=300; total time=   0.5s\n",
            "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=300; total time=   0.5s\n",
            "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=100; total time=   0.2s\n",
            "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=100; total time=   0.2s\n",
            "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=100; total time=   0.2s\n",
            "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=100; total time=   0.2s\n",
            "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=100; total time=   0.2s\n",
            "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=200; total time=   0.4s\n",
            "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=200; total time=   0.4s\n",
            "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=200; total time=   0.4s\n",
            "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=200; total time=   0.3s\n",
            "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=200; total time=   0.3s\n",
            "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=300; total time=   0.4s\n",
            "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=300; total time=   0.4s\n",
            "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=300; total time=   0.4s\n",
            "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=300; total time=   0.4s\n",
            "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=300; total time=   0.4s\n",
            "[CV] END .learning_rate=0.01, max_depth=10, n_estimators=100; total time=   0.2s\n",
            "[CV] END .learning_rate=0.01, max_depth=10, n_estimators=100; total time=   0.2s\n",
            "[CV] END .learning_rate=0.01, max_depth=10, n_estimators=100; total time=   0.2s\n",
            "[CV] END .learning_rate=0.01, max_depth=10, n_estimators=100; total time=   0.2s\n",
            "[CV] END .learning_rate=0.01, max_depth=10, n_estimators=100; total time=   0.2s\n",
            "[CV] END .learning_rate=0.01, max_depth=10, n_estimators=200; total time=   0.4s\n",
            "[CV] END .learning_rate=0.01, max_depth=10, n_estimators=200; total time=   0.4s\n",
            "[CV] END .learning_rate=0.01, max_depth=10, n_estimators=200; total time=   0.4s\n",
            "[CV] END .learning_rate=0.01, max_depth=10, n_estimators=200; total time=   0.4s\n",
            "[CV] END .learning_rate=0.01, max_depth=10, n_estimators=200; total time=   0.4s\n",
            "[CV] END .learning_rate=0.01, max_depth=10, n_estimators=300; total time=   0.6s\n",
            "[CV] END .learning_rate=0.01, max_depth=10, n_estimators=300; total time=   0.6s\n",
            "[CV] END .learning_rate=0.01, max_depth=10, n_estimators=300; total time=   0.6s\n",
            "[CV] END .learning_rate=0.01, max_depth=10, n_estimators=300; total time=   0.6s\n",
            "[CV] END .learning_rate=0.01, max_depth=10, n_estimators=300; total time=   0.6s\n",
            "[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=100; total time=   0.1s\n",
            "[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=100; total time=   0.1s\n",
            "[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=100; total time=   0.1s\n",
            "[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=100; total time=   0.1s\n",
            "[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=100; total time=   0.1s\n",
            "[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=200; total time=   0.2s\n",
            "[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=200; total time=   0.2s\n",
            "[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=200; total time=   0.2s\n",
            "[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=200; total time=   0.3s\n",
            "[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=200; total time=   0.3s\n",
            "[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=300; total time=   0.5s\n",
            "[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=300; total time=   0.5s\n",
            "[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=300; total time=   0.5s\n",
            "[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=300; total time=   0.5s\n",
            "[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=300; total time=   0.5s\n",
            "[CV] END ..learning_rate=0.05, max_depth=5, n_estimators=100; total time=   0.2s\n",
            "[CV] END ..learning_rate=0.05, max_depth=5, n_estimators=100; total time=   0.2s\n",
            "[CV] END ..learning_rate=0.05, max_depth=5, n_estimators=100; total time=   0.2s\n",
            "[CV] END ..learning_rate=0.05, max_depth=5, n_estimators=100; total time=   0.2s\n",
            "[CV] END ..learning_rate=0.05, max_depth=5, n_estimators=100; total time=   0.1s\n",
            "[CV] END ..learning_rate=0.05, max_depth=5, n_estimators=200; total time=   0.3s\n",
            "[CV] END ..learning_rate=0.05, max_depth=5, n_estimators=200; total time=   0.3s\n",
            "[CV] END ..learning_rate=0.05, max_depth=5, n_estimators=200; total time=   0.3s\n",
            "[CV] END ..learning_rate=0.05, max_depth=5, n_estimators=200; total time=   0.3s\n",
            "[CV] END ..learning_rate=0.05, max_depth=5, n_estimators=200; total time=   0.3s\n",
            "[CV] END ..learning_rate=0.05, max_depth=5, n_estimators=300; total time=   0.4s\n",
            "[CV] END ..learning_rate=0.05, max_depth=5, n_estimators=300; total time=   0.4s\n",
            "[CV] END ..learning_rate=0.05, max_depth=5, n_estimators=300; total time=   0.4s\n",
            "[CV] END ..learning_rate=0.05, max_depth=5, n_estimators=300; total time=   0.4s\n",
            "[CV] END ..learning_rate=0.05, max_depth=5, n_estimators=300; total time=   0.4s\n",
            "[CV] END .learning_rate=0.05, max_depth=10, n_estimators=100; total time=   0.2s\n",
            "[CV] END .learning_rate=0.05, max_depth=10, n_estimators=100; total time=   0.2s\n",
            "[CV] END .learning_rate=0.05, max_depth=10, n_estimators=100; total time=   0.2s\n",
            "[CV] END .learning_rate=0.05, max_depth=10, n_estimators=100; total time=   0.2s\n",
            "[CV] END .learning_rate=0.05, max_depth=10, n_estimators=100; total time=   0.2s\n",
            "[CV] END .learning_rate=0.05, max_depth=10, n_estimators=200; total time=   0.4s\n",
            "[CV] END .learning_rate=0.05, max_depth=10, n_estimators=200; total time=   0.4s\n",
            "[CV] END .learning_rate=0.05, max_depth=10, n_estimators=200; total time=   0.4s\n",
            "[CV] END .learning_rate=0.05, max_depth=10, n_estimators=200; total time=   0.4s\n",
            "[CV] END .learning_rate=0.05, max_depth=10, n_estimators=200; total time=   0.4s\n",
            "[CV] END .learning_rate=0.05, max_depth=10, n_estimators=300; total time=   0.6s\n",
            "[CV] END .learning_rate=0.05, max_depth=10, n_estimators=300; total time=   0.7s\n",
            "[CV] END .learning_rate=0.05, max_depth=10, n_estimators=300; total time=   0.6s\n",
            "[CV] END .learning_rate=0.05, max_depth=10, n_estimators=300; total time=   0.6s\n",
            "[CV] END .learning_rate=0.05, max_depth=10, n_estimators=300; total time=   0.7s\n",
            "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.2s\n",
            "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.2s\n",
            "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.2s\n",
            "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.2s\n",
            "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.2s\n",
            "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=200; total time=   0.3s\n",
            "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=200; total time=   0.4s\n",
            "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=200; total time=   0.3s\n",
            "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=200; total time=   0.4s\n",
            "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=200; total time=   0.3s\n",
            "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=300; total time=   0.5s\n",
            "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=300; total time=   0.4s\n",
            "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=300; total time=   0.3s\n",
            "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=300; total time=   0.3s\n",
            "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=300; total time=   0.3s\n",
            "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=100; total time=   0.1s\n",
            "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=100; total time=   0.1s\n",
            "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=100; total time=   0.2s\n",
            "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=100; total time=   0.1s\n",
            "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=100; total time=   0.1s\n",
            "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=200; total time=   0.3s\n",
            "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=200; total time=   0.3s\n",
            "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=200; total time=   0.3s\n",
            "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=200; total time=   0.3s\n",
            "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=200; total time=   0.3s\n",
            "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=300; total time=   0.4s\n",
            "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=300; total time=   0.4s\n",
            "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=300; total time=   0.4s\n",
            "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=300; total time=   0.4s\n",
            "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=300; total time=   0.4s\n",
            "[CV] END ..learning_rate=0.1, max_depth=10, n_estimators=100; total time=   0.2s\n",
            "[CV] END ..learning_rate=0.1, max_depth=10, n_estimators=100; total time=   0.2s\n",
            "[CV] END ..learning_rate=0.1, max_depth=10, n_estimators=100; total time=   0.2s\n",
            "[CV] END ..learning_rate=0.1, max_depth=10, n_estimators=100; total time=   0.2s\n",
            "[CV] END ..learning_rate=0.1, max_depth=10, n_estimators=100; total time=   0.2s\n",
            "[CV] END ..learning_rate=0.1, max_depth=10, n_estimators=200; total time=   0.4s\n",
            "[CV] END ..learning_rate=0.1, max_depth=10, n_estimators=200; total time=   0.4s\n",
            "[CV] END ..learning_rate=0.1, max_depth=10, n_estimators=200; total time=   0.5s\n",
            "[CV] END ..learning_rate=0.1, max_depth=10, n_estimators=200; total time=   0.4s\n",
            "[CV] END ..learning_rate=0.1, max_depth=10, n_estimators=200; total time=   0.4s\n",
            "[CV] END ..learning_rate=0.1, max_depth=10, n_estimators=300; total time=   0.6s\n",
            "[CV] END ..learning_rate=0.1, max_depth=10, n_estimators=300; total time=   0.7s\n",
            "[CV] END ..learning_rate=0.1, max_depth=10, n_estimators=300; total time=   1.0s\n",
            "[CV] END ..learning_rate=0.1, max_depth=10, n_estimators=300; total time=   1.0s\n",
            "[CV] END ..learning_rate=0.1, max_depth=10, n_estimators=300; total time=   1.1s\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "X has 1 features, but GradientBoostingRegressor is expecting 4 features as input.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-4693bd435d07>\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Predict and evaluate MAE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mmae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Final MAE after Grid Search: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmae\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   2125\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2126\u001b[0m         \"\"\"\n\u001b[0;32m-> 2127\u001b[0;31m         X = self._validate_data(\n\u001b[0m\u001b[1;32m   2128\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2129\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcheck_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ensure_2d\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 654\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_n_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_in_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    444\u001b[0m                 \u001b[0;34mf\"X has {n_features} features, but {self.__class__.__name__} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m                 \u001b[0;34mf\"is expecting {self.n_features_in_} features as input.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: X has 1 features, but GradientBoostingRegressor is expecting 4 features as input."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Apply polynomial transformation to both training and test data\n",
        "poly = PolynomialFeatures(degree=3)\n",
        "\n",
        "# Transform training data\n",
        "X_train_poly = poly.fit_transform(X_train)\n",
        "\n",
        "# Transform test data using the same polynomial features transformation\n",
        "X_test_poly = poly.transform(X_test)\n",
        "\n",
        "# Gradient Boosting model with Grid Search for tuning\n",
        "params = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [3, 5, 10],\n",
        "    'learning_rate': [0.01, 0.05, 0.1]\n",
        "}\n",
        "\n",
        "gb_model = GradientBoostingRegressor()\n",
        "grid_search = GridSearchCV(gb_model, param_grid=params, cv=5, scoring='neg_mean_absolute_error', verbose=2)\n",
        "grid_search.fit(X_train_poly, y_train)\n",
        "\n",
        "# Best parameters from grid search\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Predict and evaluate MAE\n",
        "y_pred = best_model.predict(X_test_poly)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(\"Final MAE after Grid Search: \", mae)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpqrDIGx3E0Q",
        "outputId": "f49da8e7-00fe-437a-cd48-3665a84531a4"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
            "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=100; total time=   0.2s\n",
            "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=100; total time=   0.2s\n",
            "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=100; total time=   0.2s\n",
            "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=100; total time=   0.2s\n",
            "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=100; total time=   0.3s\n",
            "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=200; total time=   0.4s\n",
            "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=200; total time=   0.3s\n",
            "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=200; total time=   0.4s\n",
            "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=200; total time=   0.3s\n",
            "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=200; total time=   0.3s\n",
            "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=300; total time=   0.8s\n",
            "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=300; total time=   1.0s\n",
            "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=300; total time=   1.5s\n",
            "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=300; total time=   0.8s\n",
            "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=300; total time=   0.6s\n",
            "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=100; total time=   0.2s\n",
            "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=100; total time=   0.2s\n",
            "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=100; total time=   0.2s\n",
            "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=100; total time=   0.2s\n",
            "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=100; total time=   0.4s\n",
            "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=200; total time=   0.5s\n",
            "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=200; total time=   0.5s\n",
            "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=200; total time=   0.8s\n",
            "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=200; total time=   0.9s\n",
            "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=200; total time=   0.9s\n",
            "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=300; total time=   1.7s\n",
            "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=300; total time=   1.0s\n",
            "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=300; total time=   0.7s\n",
            "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=300; total time=   1.0s\n",
            "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=300; total time=   0.5s\n",
            "[CV] END .learning_rate=0.01, max_depth=10, n_estimators=100; total time=   0.2s\n",
            "[CV] END .learning_rate=0.01, max_depth=10, n_estimators=100; total time=   0.2s\n",
            "[CV] END .learning_rate=0.01, max_depth=10, n_estimators=100; total time=   0.2s\n",
            "[CV] END .learning_rate=0.01, max_depth=10, n_estimators=100; total time=   0.2s\n",
            "[CV] END .learning_rate=0.01, max_depth=10, n_estimators=100; total time=   0.2s\n",
            "[CV] END .learning_rate=0.01, max_depth=10, n_estimators=200; total time=   0.4s\n",
            "[CV] END .learning_rate=0.01, max_depth=10, n_estimators=200; total time=   0.4s\n",
            "[CV] END .learning_rate=0.01, max_depth=10, n_estimators=200; total time=   0.4s\n",
            "[CV] END .learning_rate=0.01, max_depth=10, n_estimators=200; total time=   0.4s\n",
            "[CV] END .learning_rate=0.01, max_depth=10, n_estimators=200; total time=   0.4s\n",
            "[CV] END .learning_rate=0.01, max_depth=10, n_estimators=300; total time=   0.6s\n",
            "[CV] END .learning_rate=0.01, max_depth=10, n_estimators=300; total time=   0.6s\n",
            "[CV] END .learning_rate=0.01, max_depth=10, n_estimators=300; total time=   0.6s\n",
            "[CV] END .learning_rate=0.01, max_depth=10, n_estimators=300; total time=   0.6s\n",
            "[CV] END .learning_rate=0.01, max_depth=10, n_estimators=300; total time=   0.6s\n",
            "[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=100; total time=   0.1s\n",
            "[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=100; total time=   0.1s\n",
            "[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=100; total time=   0.1s\n",
            "[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=100; total time=   0.1s\n",
            "[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=100; total time=   0.1s\n",
            "[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=200; total time=   0.2s\n",
            "[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=200; total time=   0.2s\n",
            "[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=200; total time=   0.3s\n",
            "[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=200; total time=   0.3s\n",
            "[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=200; total time=   0.3s\n",
            "[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=300; total time=   0.5s\n",
            "[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=300; total time=   0.5s\n",
            "[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=300; total time=   0.5s\n",
            "[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=300; total time=   0.5s\n",
            "[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=300; total time=   0.5s\n",
            "[CV] END ..learning_rate=0.05, max_depth=5, n_estimators=100; total time=   0.2s\n",
            "[CV] END ..learning_rate=0.05, max_depth=5, n_estimators=100; total time=   0.2s\n",
            "[CV] END ..learning_rate=0.05, max_depth=5, n_estimators=100; total time=   0.1s\n",
            "[CV] END ..learning_rate=0.05, max_depth=5, n_estimators=100; total time=   0.1s\n",
            "[CV] END ..learning_rate=0.05, max_depth=5, n_estimators=100; total time=   0.1s\n",
            "[CV] END ..learning_rate=0.05, max_depth=5, n_estimators=200; total time=   0.3s\n",
            "[CV] END ..learning_rate=0.05, max_depth=5, n_estimators=200; total time=   0.3s\n",
            "[CV] END ..learning_rate=0.05, max_depth=5, n_estimators=200; total time=   0.3s\n",
            "[CV] END ..learning_rate=0.05, max_depth=5, n_estimators=200; total time=   0.3s\n",
            "[CV] END ..learning_rate=0.05, max_depth=5, n_estimators=200; total time=   0.3s\n",
            "[CV] END ..learning_rate=0.05, max_depth=5, n_estimators=300; total time=   0.4s\n",
            "[CV] END ..learning_rate=0.05, max_depth=5, n_estimators=300; total time=   0.4s\n",
            "[CV] END ..learning_rate=0.05, max_depth=5, n_estimators=300; total time=   0.4s\n",
            "[CV] END ..learning_rate=0.05, max_depth=5, n_estimators=300; total time=   0.4s\n",
            "[CV] END ..learning_rate=0.05, max_depth=5, n_estimators=300; total time=   0.4s\n",
            "[CV] END .learning_rate=0.05, max_depth=10, n_estimators=100; total time=   0.2s\n",
            "[CV] END .learning_rate=0.05, max_depth=10, n_estimators=100; total time=   0.2s\n",
            "[CV] END .learning_rate=0.05, max_depth=10, n_estimators=100; total time=   0.2s\n",
            "[CV] END .learning_rate=0.05, max_depth=10, n_estimators=100; total time=   0.2s\n",
            "[CV] END .learning_rate=0.05, max_depth=10, n_estimators=100; total time=   0.2s\n",
            "[CV] END .learning_rate=0.05, max_depth=10, n_estimators=200; total time=   0.4s\n",
            "[CV] END .learning_rate=0.05, max_depth=10, n_estimators=200; total time=   0.4s\n",
            "[CV] END .learning_rate=0.05, max_depth=10, n_estimators=200; total time=   0.4s\n",
            "[CV] END .learning_rate=0.05, max_depth=10, n_estimators=200; total time=   0.4s\n",
            "[CV] END .learning_rate=0.05, max_depth=10, n_estimators=200; total time=   0.4s\n",
            "[CV] END .learning_rate=0.05, max_depth=10, n_estimators=300; total time=   0.6s\n",
            "[CV] END .learning_rate=0.05, max_depth=10, n_estimators=300; total time=   0.6s\n",
            "[CV] END .learning_rate=0.05, max_depth=10, n_estimators=300; total time=   0.6s\n",
            "[CV] END .learning_rate=0.05, max_depth=10, n_estimators=300; total time=   0.6s\n",
            "[CV] END .learning_rate=0.05, max_depth=10, n_estimators=300; total time=   0.9s\n",
            "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.2s\n",
            "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.2s\n",
            "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.2s\n",
            "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.2s\n",
            "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.2s\n",
            "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=200; total time=   0.3s\n",
            "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=200; total time=   0.3s\n",
            "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=200; total time=   0.3s\n",
            "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=200; total time=   0.3s\n",
            "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=200; total time=   0.3s\n",
            "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=300; total time=   0.5s\n",
            "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=300; total time=   0.4s\n",
            "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=300; total time=   0.3s\n",
            "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=300; total time=   0.3s\n",
            "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=300; total time=   0.3s\n",
            "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=100; total time=   0.1s\n",
            "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=100; total time=   0.2s\n",
            "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=100; total time=   0.1s\n",
            "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=100; total time=   0.1s\n",
            "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=100; total time=   0.1s\n",
            "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=200; total time=   0.3s\n",
            "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=200; total time=   0.3s\n",
            "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=200; total time=   0.3s\n",
            "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=200; total time=   0.3s\n",
            "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=200; total time=   0.3s\n",
            "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=300; total time=   0.4s\n",
            "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=300; total time=   0.4s\n",
            "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=300; total time=   0.4s\n",
            "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=300; total time=   0.4s\n",
            "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=300; total time=   0.4s\n",
            "[CV] END ..learning_rate=0.1, max_depth=10, n_estimators=100; total time=   0.2s\n",
            "[CV] END ..learning_rate=0.1, max_depth=10, n_estimators=100; total time=   0.2s\n",
            "[CV] END ..learning_rate=0.1, max_depth=10, n_estimators=100; total time=   0.2s\n",
            "[CV] END ..learning_rate=0.1, max_depth=10, n_estimators=100; total time=   0.2s\n",
            "[CV] END ..learning_rate=0.1, max_depth=10, n_estimators=100; total time=   0.2s\n",
            "[CV] END ..learning_rate=0.1, max_depth=10, n_estimators=200; total time=   0.4s\n",
            "[CV] END ..learning_rate=0.1, max_depth=10, n_estimators=200; total time=   0.4s\n",
            "[CV] END ..learning_rate=0.1, max_depth=10, n_estimators=200; total time=   0.4s\n",
            "[CV] END ..learning_rate=0.1, max_depth=10, n_estimators=200; total time=   0.4s\n",
            "[CV] END ..learning_rate=0.1, max_depth=10, n_estimators=200; total time=   0.4s\n",
            "[CV] END ..learning_rate=0.1, max_depth=10, n_estimators=300; total time=   0.7s\n",
            "[CV] END ..learning_rate=0.1, max_depth=10, n_estimators=300; total time=   0.7s\n",
            "[CV] END ..learning_rate=0.1, max_depth=10, n_estimators=300; total time=   1.0s\n",
            "[CV] END ..learning_rate=0.1, max_depth=10, n_estimators=300; total time=   1.0s\n",
            "[CV] END ..learning_rate=0.1, max_depth=10, n_estimators=300; total time=   1.1s\n",
            "Final MAE after Grid Search:  160.57913491491442\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale the input Y = 2.2 using the same scaler as during training\n",
        "Y_input_scaled = scaler_X.transform([[2.2]])\n",
        "\n",
        "# Predict the output using the trained model\n",
        "X_pred_scaled = model.predict(Y_input_scaled)\n",
        "\n",
        "# Inverse scale the predicted output to get the result in the original scale\n",
        "X_pred = scaler_y.inverse_transform(X_pred_scaled)\n",
        "\n",
        "print(f\"Predicted solar irradiance (X) for LDR value 2.2: {X_pred[0][0]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SdYvTjy3hni",
        "outputId": "d51558dd-9586-41a8-b3b7-b777212316ce"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step\n",
            "Predicted solar irradiance (X) for LDR value 2.2: 344.8267822265625\n"
          ]
        }
      ]
    }
  ]
}